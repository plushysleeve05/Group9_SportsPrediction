# -*- coding: utf-8 -*-
"""Group9_SportsPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N4KFgKzyOXMxXpexAFQuqDSA3sdyyKnN

Importing libraries
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_val_score
import xgboost as xgb
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, make_scorer, mean_absolute_error
import pickle
import joblib

"""Mounting Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""Data Preparation and Feature Extraction:


"""

players_21=pd.read_csv('/content/drive/My Drive/Colab Notebooks/players_21.csv')

players_21.describe()

players_21.info()

players_21.head()

print(players_21.shape[1])

#droping columns with more than 30% null values
threshold = 0.3
total_rows = len(players_21)
for column in players_21.columns:
  miss_percentage = players_21[column].isnull().sum()/total_rows

  if miss_percentage > threshold:
    players_21 = players_21.drop(column, axis=1)

print(players_21.shape[1])

#droping useless columns
useless_columns = ['sofifa_id', 'player_url', 'long_name', 'dob', 'short_name',
                     'body_type', 'real_face',
                   'player_face_url', 'club_logo_url',  'nation_flag_url','club_flag_url']

new_players_21 = players_21.drop(useless_columns, axis = 1)

#separating the data into numeric and textual forms:
numeric_data = pd.DataFrame()
textual_data = pd.DataFrame()

for column in new_players_21:
  if new_players_21[column].dtype in ['int64', 'float64']:
    numeric_data[column] = new_players_21[column]
  else:
    textual_data[column] = new_players_21[column]

numeric_data.info()

textual_data.info()

#imputing the numerical data
columns_with_missing_values = numeric_data.columns[numeric_data.isnull().any().tolist()]

for column in columns_with_missing_values:
   numeric_data[column].fillna(numeric_data[column].mean(), inplace=True)

#imputing the categorical data
textual_columns_with_missing_values = textual_data.columns[textual_data.isnull().any().tolist()]

for column in textual_columns_with_missing_values:
  textual_data[column].fillna(textual_data[column].mode()[0], inplace=True)

numeric_data.info()

#encoding the textual values
label_encoder = LabelEncoder()

for column in textual_data:
  textual_data[column] = label_encoder.fit_transform(textual_data[column])

textual_data

cleaned_players_21 = pd.concat([numeric_data, textual_data], axis=1)
cleaned_players_21

"""FEATURE ENGINEERING PROCESS:"""

#feature engineering with randomforestclassifier - feature importance

#choosing X and y
X = cleaned_players_21.drop('overall',axis=1)
y = cleaned_players_21['overall']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
rforrest_classifier = RandomForestClassifier(n_estimators=112, max_depth=12, criterion='entropy')

rforrest_classifier.fit(X_train, y_train)

feature_importance = rforrest_classifier.feature_importances_

for name, score in zip(X.columns, feature_importance):
  print(name,'=', score)

#sorting the feature importance
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)
feature_importance_df.iloc[0:20]

#using correlation matrix
cmatrix = pd.DataFrame(cleaned_players_21.corr())
cmatrix['overall'].iloc[0:21].sort_values(ascending = False)

#picking from the top 10 features according to the feature importance and the correlation matrix:
selected_features = ['potential', 'value_eur','age','release_clause_eur','dribbling','shooting','wage_eur'
]

##training and testing next:
#scaling
scaler = StandardScaler()
X = cleaned_players_21[selected_features]
X_scaled = scaler.fit_transform(X)
y = pd.DataFrame(cleaned_players_21['overall'])

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

"""training:"""

#Random Forrest Regressor with grid search
model = RandomForestRegressor(random_state=42)

model.fit(X_train, y_train)
initial_predictions = model.predict(X_test)
initial_rmse = np.sqrt(mean_squared_error(y_test, initial_predictions))
initial_rmse

param_grid = {
    'n_estimators': [100, 150, 200],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'max_features': ['auto', 'sqrt', 'log2']
}

grid_search = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=4)

grid_search.fit(X_train, y_train)

best_parameters = grid_search.best_params_
best_parameters

best_model = grid_search.best_estimator_
best_model

#evaluating the model with cross_val_score
score = cross_val_score(best_model,X,y,scoring='neg_mean_squared_error', cv=5)

rmse_scores = np.sqrt(-score)
rmse_scores

final_rfr_model = RandomForestRegressor(n_estimators=200, max_depth=15, min_samples_split=2, random_state=42)

final_rfr_model.fit(X_train,y_train)

final_pred=final_rfr_model.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test,final_pred))
print("Mean RMSE:", rmse)

final_rfr_model.score(X_test, y_test)

#xgboost model:
xgb_model = xgb.XGBRegressor()

xgb_model.fit(X_train, y_train)
xgb_initial_predictions = xgb_model.predict(X_test)
xgb_initial_rmse = np.sqrt(mean_squared_error(y_test, xgb_initial_predictions))
xgb_initial_rmse

"""***Hyperparameter Tuning***

"""

param_grid1 = {
    'learning_rate': [0.1, 0.01],
    'n_estimators': [100, 500],
    'max_depth': [3, 5],
    'min_child_weight': [1, 3],
    'gamma': [0, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0],
}

#root mean score error scorer
scorer = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False)
cv = KFold(n_splits=3, shuffle=True, random_state=42)
xgb_grid_search = GridSearchCV(xgb_model, param_grid=param_grid1, scoring=scorer,cv=cv)

xgb_grid_search.fit(X_train, y_train)

xgb_best_parameters = xgb_grid_search.best_params_
xgb_best_parameters

#best xgb model
xgb_best_model = xgb_grid_search.best_estimator_
xgb_best_model

xgb_cv_scores = cross_val_score(xgb_best_model, X, y, cv=cv, scoring=scorer)
xgb_rmse_scores = -xgb_cv_scores

print("RMSE Scores for Each Fold:", xgb_rmse_scores)

#final xgb model
final_xgb_model = xgb_best_model

#training the final model
final_xgb_model.fit(X_train,y_train)

#xgb_prediction
xgb_prediction = final_xgb_model.predict(X_test)
xgb_prediction

mean_absolute_error(xgb_prediction,y_test)

xgb_mean_rmse = np.mean(xgb_rmse_scores)
print("Mean RMSE:", xgb_mean_rmse)

final_xgb_model.score(X_test, y_test)

#gradient boost
gb_model = GradientBoostingRegressor()

gb_model.fit(X_train, y_train)
gb_initial_predictions = gb_model.predict(X_test)
gb_initial_rmse = np.sqrt(mean_squared_error(y_test, gb_initial_predictions))
gb_initial_rmse

"""**Hyperparameter tuning:**"""

parameter_grid = {
    'n_estimators': [100, 150, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
}

gb_scorer = scorer #from the xgboost regressor

cv = cv # from xgboost cv

gb_grid_search = GridSearchCV(gb_model, parameter_grid, scoring=gb_scorer, cv=cv)

gb_grid_search.fit(X_train,y_train)

gb_best_parameters = gb_grid_search.best_params_
gb_best_parameters

#best model
gb_best_model = gb_grid_search.best_estimator_

#final GradientBoostingRegressor Model
final_gb_model = gb_best_model

gb_prediction = final_gb_model.predict(X_test)
gb_prediction

gb_cv_scores = cross_val_score(final_gb_model, X, y, cv=cv, scoring=scorer)

gb_rmse_score = -gb_cv_scores
gb_rmse_score

#mean root mean square error for GradientboostingRegression
mean_rmse = np.mean(gb_rmse_score)
print("Mean RMSE:", mean_rmse)

final_gb_model.score(X_test, y_test)



"""**TESTING WITH UNSEEN DATA (PLAYERS 22)** WITH trained MODEL: ***GradientBoostingRegressor Model, XGBOOST, RandomForrestRegressor***

Cleaning the players 22
"""

players_22=pd.read_csv('/content/drive/My Drive/Colab Notebooks/players_22.csv')

print(players_22.shape[1])

#droping columns with more than 30% null values
threshold = 0.3
total_rows = len(players_22)
for column in players_22.columns:
  miss_percentage = players_22[column].isnull().sum()/total_rows

  if miss_percentage > threshold:
    players_22 = players_22.drop(column, axis=1)

print(players_22.shape[1])

#dropping useless columns
useless_columns22 = ['sofifa_id', 'player_url', 'long_name', 'dob', 'short_name',
                     'body_type', 'real_face',
                   'player_face_url', 'club_logo_url',  'nation_flag_url','club_flag_url']

new_players_22 = players_22.drop(useless_columns22, axis = 1)

new_players_22.head()

#separating the data into numeric and textual forms:
numeric_data22 = pd.DataFrame()
textual_data22 = pd.DataFrame()

for column in new_players_22:
  if new_players_22[column].dtype in ['int64', 'float64']:
    numeric_data22[column] = new_players_22[column]
  else:
    textual_data22[column] = new_players_22[column]

numeric_data22.isnull().sum()

textual_data22.isnull().sum()

#imputing the numerical data
columns_with_missing_values22 = numeric_data22.columns[numeric_data22.isnull().any().tolist()]

for column in columns_with_missing_values22:
   numeric_data22[column].fillna(numeric_data22[column].mean(), inplace=True)

#imputing the categorical data
textual_columns_with_missing_values22 = textual_data22.columns[textual_data22.isnull().any().tolist()]

for column in textual_columns_with_missing_values22:
  textual_data22[column].fillna(textual_data22[column].mode()[0], inplace=True)

#encoding the textual values
label_encoder22 = LabelEncoder()

for column in textual_data22:
  textual_data22[column] = label_encoder22.fit_transform(textual_data22[column])

cleaned_players_22 = pd.concat([numeric_data22, textual_data22], axis=1)
cleaned_players_22

features = cleaned_players_22[selected_features]
features

features_scaled = scaler.fit_transform(features)

new_data_predgb = pd.DataFrame(final_gb_model.predict(features_scaled))
new_data_predgb

new_data_predxgb = pd.DataFrame(final_xgb_model.predict(features_scaled))
new_data_predxgb

new_data_pred = pd.DataFrame(final_rfr_model.predict(features_scaled))
new_data_pred

cleaned_players_22['overall']

#evaluating the model's performance on the new data
ground_truth = cleaned_players_22['overall']
mean_squared_error(ground_truth, new_data_pred)

prediction_rootmse = np.sqrt(mean_squared_error(ground_truth, new_data_predxgb))
prediction_rootmse

"""SAVING THE TRAINED AND TESTED MODEL:"""

model_directory = '/content/drive/MyDrive/Colab Notebooks/final_xgb_model.pk1'

with open(model_directory, 'wb') as file:
  pickle.dump(final_xgb_model, file)

joblib.dump(scaler, '/content/drive/MyDrive/Colab Notebooks/scaler.pkl')